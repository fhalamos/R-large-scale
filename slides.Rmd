---
title: "Large-scale data analysis in R"
author: Peter Carbonetto
output:
  beamer_presentation:
    template: docs/beamer.tex
    keep_tex: false
    fig_caption: false
    pandoc_args: "--highlight-style=pygments"
---

```{r knitr-options, echo=FALSE}
knitr::opts_chunk$set(results = "hide",fig.show = "hide",
                      message = FALSE,warning = FALSE)
```

Workshop aims
=============

1. Develop essential skills for large-scale data analyis in R, and
apply these skills to a large-ish data set.

2. Learn how to use R **non-interactively** within a high-performance
computing (HPC) environment.

3. Assess memory needs.

4. Make efficient use of memory.

5. Speed up your R analyses by...

    + using simple parallelization techniques.

    + interfacing to C code (Rcpp).

6. Learn through **live coding**---this includes learning from our
mistakes!

It's your choice
================

You may choose to...

+ Use R on the RCC cluster.

+ Use R on your laptop.

+ Pair up with your neighbour.

+ Follow what I do on the projector.

Please note:

+ Please use R, not RStudio.
 
+ Some examples will only work on the RCC cluster.

Software we will use today
==========================

1. R

2. Python 3.x (for assessing memory usage).

3. Slurm (job scheduler on RCC cluster).

4. R packages **data.table**, **parallel** and **Rcpp**.

*These software and R packages are already installed on the RCC cluster.*

The "large-ish" data set
========================

+ RegMap data: genetic and ecological data on *Arabidopsis thaliana*
in a range of climates.

+ From Joy Bergelson's lab at the University of Chicago.

+ See [Hancock et al (2011) *Science* 334, 83--86][hancock-2011].

Outline of workshop
===================

1. Initial setup.

2. Vignettes:

    + Vignette #1: Importing a large data set.

    + Vignette #2: Automating analysis of a large data set.

    + Vignette #3: Speeding up operations on large matrices.

    + Vignette #4: Multithreaded computing with "parLapply".

    + Vignette #5: Using Rcpp to improve performance.

Preliminaries
=============

+ WiFi.

+ Power outlets.

+ Reading what I type.

+ Pace & questions (e.g., keyboard shortcuts).

+ Getting stuck.

Download workshop packet
========================

Once you have connected to a midway2 login node, download the workshop
packet (a git repository) to your home directory on the cluster
(**note:** there are no spaces in the URL):

```bash
cd ~
git clone https://github.com/rcc-uchicago/
  R-large-scale.git
```

*Optional:* Download the repository to your laptop.

What's included in the workshop packet
======================================

+ **slides.pdf:** These slides.

+ **slides.Rmd:** R Markdown source used to generate these
  slides.

+ **.R** files: R scripts we will run in the examples.

+ **.sbatch** files: sbatch scripts we will run to allocate resources
  for our analyses on the cluster.

+ **monitor_memory.py:** Python script to assess memory usage.

+ **set_slurm_env.sh:** Shell commands to configure Slurm.

Retrieve the data
=================

Copy and uncompress the data to your home directory:

```bash
cd ~/R-large-scale
cp ~pcarbo/share/regmap.tar.gz .
tar zxvf regmap.tar.gz
```

Connect to a midway2 compute node
=================================

Request access to a midway2 ("Broadwell") compute node with 8 CPUs and
18 GB of memory:

```bash
screen -S workshop
sinteractive --partition=broadwl \
  --reservation=workshop --cpus-per-task=8 \
  --mem=18G --time=3:00:00 
echo $HOSTNAME
```

Launch R
========

Start up the R interactive environment:

```bash
module load R/3.5.1
R
```

Check your R environment
========================

Check that your R environment is set up correctly:

```{r check-environment}
sessionInfo()
ls()
getwd()
```


Launch a second console to monitor computations
===============================================

*Add instructions here.*

Vignette #1: Importing a large data set
=======================================

Our first aim is to import the RegMap genotype data into R. Let's
attempt this using `read.csv`:

```{r import-data-readcsv, eval=FALSE}
geno <- read.csv("geno.csv",check.names = FALSE)
```


Import genotype data using `data.table`
=======================================

Let's try again using the **data.table** package:

```{r import-data-fread, eval=FALSE}
# install.packages("data.table")
library(data.table) 
geno <- fread("geno.csv",sep = ",",header = TRUE)
class(geno) <- "data.frame"
```


Timing the data import step
===========================

Let's document how long it took to import the data using "fread":

```{r time-import-data-fread, eval=FALSE}
timing <- system.time(
  geno <- fread("geno.csv",sep = ",",
                header = TRUE))
class(geno) <- "data.frame"
```


Vignette #2: Automating analysis of a large data set
====================================================

A common step in genetic analysis is to examine the distribution of
minor allele frequencies. Since the RegMap genotypes are encoded as
allele counts, the allele frequencies are simply the means of each
table column:

```{r compute-mafs, eval=FALSE}
maf <- sapply(geno,mean)
maf <- pmin(maf,1 - maf)
```

Let's now summarize the minor allele frequencies:

```{r summarize-mafs, eval=FALSE}
summary(maf)
```

Automating the analysis using Rscript
=====================================

Now let's quit R, and re-run all the steps of the analysis:

```bash
Rscript summarize_regmap_mafs.R
```


Automating environment setup and resource allocation
====================================================

Rscript automates all the steps of the analysis *in the R
environment*, but it does not automate the steps prior to running the
R code. In a separate midway2 connection, run this command to re-run
the environment setup and analysis:

```bash
sbatch summarize_regmap_mafs.sbatch
```

Run these commands to check the status of your analysis:

```bash
source set_slurm_env.sh
squeue --user=<cnetid>
sacct --user=<cnetid>
```


Automating the analysis for many data sets
==========================================

Suppose now you want to repeat your analysis for several other
(similar) genetic data sets. To reduce effort, you could re-write your
R script to make it more *generic* by accepting the name of the data
file to use as a command-line argument:

```bash
Rscript summarize_mafs.R geno.csv
```

Likewise, we can also implement an sbatch script that will accept the
data file name as a command-line argument:

```bash
sbatch summarize_mafs.sbatch geno.csv
```


Vignette #3: Speeding up large-scale matrix operations
======================================================

*Add text here.*

```{r crossprod, eval=FALSE}
geno <- as.matrix(geno)
storage.mode(geno) <- "double"
K <- tcrossprod(geno)
```

Let's determine how long it takes to compute this matrix:

```{r crossprod-timing, eval=FALSE}
timing <- system.time(
  K <- tcrossprod(geno))
```


Exploiting multithreaded OpenBLAS
=================================

Re-run the computation of the kinship matrix.

```bash
Rscript compute_regmap_kinship.R
```

Next, run this command to tell OpenBLAS to use 2 CPUs, and run
the computation again:

```bash
export OPENBLAS_NUM_THREADS=2
Rscript compute_regmap_kinship.R
```

Now try using 4 and 8 threads. *Do you obtain additional speedups?*


Vignette #4: Multithreaded computations with parLapply
======================================================

*Implement this vignette, in which we use parLapply to parallize
computation of the association statistics.*

Vignette #5: Using Rcpp to reduce memory & effort
=================================================

*Implement this vignette.*

Another very common step in analysis of genetic data is to center and
scale ("normalize") the columns of the genotype matrix. (This is often
done before running PCA.) In R, the "scale" function accomplishes
this.

```R
geno <- scale(geno)
```

[hancock-2011]: http://dx.doi.org/10.1126/science.1209244 
