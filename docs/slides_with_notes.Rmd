---
title: "Large-scale data analysis in R"
author: Peter Carbonetto
output:
  beamer_presentation:
    template: docs/beamer.tex
    keep_tex: false
    fig_caption: false
    pandoc_args: "--highlight-style=pygments"
---

```{r knitr-options, echo=FALSE}
knitr::opts_chunk$set(results = "hide",fig.show = "hide",
                      message = FALSE,warning = FALSE)
```

Workshop aims
=============

1. Develop skills for large-scale data analyis in R, and apply these
skills to a large-ish data set.

2. Learn how to use R **non-interactively** within a high-performance
computing (HPC) environment.

3. Assess memory needs, and make efficient use of memory.

4. Speeding up your R analyses by...

    + Using simple parallelization techniques.

    + Interfacing to C code.

5. Learn through **live coding**---this includes learning from our
mistakes!

It's your choice
================

You may choose to...

+ Use R on the RCC cluster.

+ Use R on your laptop.

+ Pair up with your neighbour.

+ Follow what I do on the projector.

Please note:

+ You should use R, not RStudio.
 
+ Some of the examples will not work on your laptop.

Software we will use today
==========================

1. R

2. Python 3.x (used to measure memory usage).

3. Slurm (job scheduler on RCC cluster).

4. R packages **data.table**, ...

*These programs and R packages are already installed on the cluster.*

The large data set
==================

+ **RegMap data:** genetic and ecological data on *Arabidopsis thaliana*
in a range of climates.

+ From Joy Bergelson's lab at the University of Chicago.

+ See [Hancock et al (2011) *Science* 334, 83--86][hancock-2011].

Outline of workshop
===================

1. Initial setup.

2. Vignettes:

    + *Add vignettes here.*

Initial setup
=============

+ WiFi.

+ Power outlets.

+ Workshop packet.

+ Reading what I type.

+ Pace & questions (e.g., keyboard shortcuts).

+ Help.

Download workshop packet
========================

Once you have connected to a midway2 login node, download the workshop
packet to your home directory on the cluster (**note:** no spaces in URL):

```bash
cd $HOME
git clone https://github.com/rcc-uchicago/
  R-large-scale.git
```

*Optionally,* you may also download the repository to your laptop.

What's included in the workshop packet
======================================

+ **slides.pdf:** These slides.

+ *Describe more of the files here.*

Copy the data
=============

Copy and uncompress the data in the same location as the other
workshop files:

```bash
cd R-large-scale
cp ~pcarbo/share/regmap.tar.gz .
tar zxvf regmap.tar.gz
```

Connect to a midway2 compute node
=================================

Request access to a midway2 ("Broadwell") compute node with 8 CPUs and
18 GB of memory:

```bash
screen -S workshop
sinteractive --partition=broadwl \
  --reservation=workshop --cpus-per-task=8 \
  --mem=18G --time=3:00:00 
echo $HOSTNAME
```

Set up your shell environment
=============================

*Add instructions here.*

Launch R
========

Start up an R interactive environment:

```bash
module load R/3.5.1
R
```

Check your R environment
========================

Check that your R environment is set up correctly:

```{r check-environment}
sessionInfo()
ls()
getwd()
```

> Instructor notes: If `ls()` does not return `character(0)`, clear
> your environment by running `rm(list = ls())`.

Launch a second console to monitor computations
===============================================

*Add instructions here.*

Vignette #1: Importing a large data set
=======================================

Our first aim is to import the RegMap genotype data into R. Let's
attempt this using `read.csv`:

```{r import-data-readcsv}
geno <- read.csv("geno.csv",check.names = FALSE)
```

> Instructor notes: This is a very short vignette to demonstrate that
> the base functions (e.g., read.csv) are not well-suited for large
> data set.
>
> Before importing the data into R, inspect the CSV file, and run
> simple shell commands (e.g., "wc -l") to understand the data. From
> this initial inspection, we should see that it is a large binary
> matrix.

Import genotype data using `data.table`
=======================================

Let's try again using the **data.table** package:

```{r import-data-fread}
# install.packages("data.table")
library(data.table) 
geno <- fread("geno.csv",sep = ",",header = TRUE)
class(geno) <- "data.frame"
```

> Instructor notes: The "fread" function returns an object of class
> "data.table". I prefer working with the (much more standard) data
> frame, so I convert it.

Timing the data import step
===========================

Let's document how long it took to import the data using "fread":

```{r time-import-data-fread}
timing <- system.time(
  geno <- fread("geno.csv",sep = ",",
                header = TRUE))
class(geno) <- "data.frame"
```

> Instructor notes: Before continuing to next vignette, ask
> participants to determine how much memory is required to store the
> genotype data in R. I suggest `print(object.size(geno),units="Mb")`.

Vignette #2: Automating analysis of a large data set
====================================================

A common step in genetic analysis is to examine the distribution of
minor allele frequencies. Since the RegMap genotypes are encoded as
allele counts, the allele frequencies are simply the means of each
table column:

```{r compute-mafs}
maf <- sapply(geno,mean)
maf <- pmin(maf,1 - maf)
```

Let's now summarize the minor allele frequencies:

```{r summarize-mafs}
summary(maf)
```

Automating the analysis using Rscript
=====================================

Now let's quit R, and re-run all the steps of the analysis:

```bash
Rscript summarize_regmap_mafs.R
```

> Instructor notes: Before running the script, inspect the code.

Automating environment setup and resource allocation
====================================================

Rscript automates all the steps of the analysis *in the R
environment*, but it does not automate the steps prior to running the
R code. In a separate midway2 connection, run this command to re-run
the environment setup and analysis:

```bash
sbatch summarize_regmap_mafs.sbatch
```

Run these commands to check the status of your analysis:

```bash
source set_slurm_env.sh
squeue --user=<cnetid>
sacct --user=<cnetid>
```

> Instructor notes: Before running the sbatch script, inspect the
> code.
>
> The students should find that the sbatch script does not request
> sufficient memory to run the analysis. (Somewhere between 2 and 4 GB
> should be sufficient.) This provides motivation for one of the
> vignettes below.

Automating the analysis for many data sets
==========================================

Suppose now you want to repeat your analysis for several other
(similar) genetic data sets. To reduce effort, you could re-write your
R script to make it more *generic* by accepting the name of the data
file to use as a command-line argument:

```bash
Rscript summarize_mafs.R geno.csv
```

Likewise, we can also implement an sbatch script that will accept the
data file name as a command-line argument:

```bash
sbatch summarize_mafs.sbatch geno.csv
```

> Instructor notes: Ask the students how the sbatch script could be
> improved. In particular, how to handle (1) data sets of different
> sizes (requiring different amounts of memory), and (2) saving the
> results of the anlayses to different files?

Vignette #3: Speeding up large-scale matrix operations
======================================================

*Add text here.*

```{r crossprod}
geno <- as.matrix(geno)
storage.mode(geno) <- "double"
K <- tcrossprod(geno)
```

Let's determine how long it takes to compute this matrix:

```{r crossprod-timing}
timing <- system.time(
  K <- tcrossprod(geno))
```

> Instructor notes:

Exploiting multithreaded OpenBLAS
=================================

Re-run the computation of the kinship matrix.

```bash
Rscript compute_regmap_kinship.R
```

Next, run this command to tell OpenBLAS to use 2 CPUs, and run
the computation again:

```bash
export OPENBLAS_NUM_THREADS=2
Rscript compute_regmap_kinship.R
```

Now try using 4 and 8 threads. *Do you obtain additional speedups?*

> Instructor notes: You can also demonstrate how to use "htop" in
> separate console to monitor CPU usage.

Vignette #4: Multithreaded computations with parLapply
======================================================

*Implement this vignette, in which we use parLapply to parallize
computation of the association statistics.*

Vignette #5: Using Rcpp to reduce memory & effort
=================================================

*Implement this vignette.*

Another very common step in analysis of genetic data is to center and
scale ("normalize") the columns of the genotype matrix. (This is often
done before running PCA.) In R, the "scale" function accomplishes
this.

```R
geno <- scale(geno)
```

[hancock-2011]: http://dx.doi.org/10.1126/science.1209244 
